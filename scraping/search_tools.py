#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ” å®ç”¨æœç´¢å·¥å…·
æä¾›å¤šç§æœç´¢æ–¹æ¡ˆï¼Œç»•è¿‡åçˆ¬é™åˆ¶
"""

import requests
import json
from typing import List, Dict, Optional
import os
import sys

# æ·»åŠ è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from anti_spider_tools import get_random_headers

# ä»£ç†è®¾ç½®
PROXIES = {
    "http": "http://127.0.0.1:7890",
    "https": "http://127.0.0.1:7890",
}

class BraveSearch:
    """
    Brave Search API (æ¨è)
    éœ€è¦ API Keyï¼Œä½†ç¨³å®šå¯é 
    å…è´¹é¢åº¦ï¼š2000 queries/month
    æ³¨å†Œ: https://api.search.brave.com/
    """
    
    API_URL = "https://api.search.brave.com/res/v1/web/search"
    
    def __init__(self, api_key: str = None):
        self.api_key = api_key or os.getenv("BRAVE_API_KEY")
    
    def search(self, query: str, count: int = 10) -> List[Dict]:
        """æœç´¢"""
        if not self.api_key:
            print("âš ï¸ æœªè®¾ç½® BRAVE_API_KEYï¼Œè·³è¿‡ Brave Search")
            return []
        
        headers = {
            "Accept": "application/json",
            "Accept-Encoding": "gzip",
            "X-Subscription-Token": self.api_key,
        }
        
        params = {
            "q": query,
            "count": min(count, 20),
            "search_lang": "zh",
        }
        
        try:
            response = requests.get(
                self.API_URL,
                headers=headers,
                params=params,
                proxies=PROXIES,
                timeout=15
            )
            
            if response.status_code == 200:
                data = response.json()
                results = []
                for item in data.get("web", {}).get("results", []):
                    results.append({
                        "title": item.get("title", ""),
                        "url": item.get("url", ""),
                        "description": item.get("description", ""),
                    })
                return results
            else:
                print(f"âŒ Brave API é”™è¯¯: {response.status_code}")
                return []
                
        except Exception as e:
            print(f"âŒ Brave æœç´¢å¤±è´¥: {e}")
            return []

class SerperSearch:
    """
    Serper.dev - Google Search API
    éœ€è¦ API Key
    å…è´¹é¢åº¦ï¼š2500 queries
    æ³¨å†Œ: https://serper.dev/
    """
    
    API_URL = "https://google.serper.dev/search"
    
    def __init__(self, api_key: str = None):
        self.api_key = api_key or os.getenv("SERPER_API_KEY")
    
    def search(self, query: str, count: int = 10) -> List[Dict]:
        """æœç´¢"""
        if not self.api_key:
            return []
        
        headers = {
            "X-API-KEY": self.api_key,
            "Content-Type": "application/json",
        }
        
        payload = {
            "q": query,
            "num": min(count, 10),
        }
        
        try:
            response = requests.post(
                self.API_URL,
                headers=headers,
                json=payload,
                proxies=PROXIES,
                timeout=15
            )
            
            if response.status_code == 200:
                data = response.json()
                results = []
                for item in data.get("organic", []):
                    results.append({
                        "title": item.get("title", ""),
                        "url": item.get("link", ""),
                        "description": item.get("snippet", ""),
                    })
                return results
            else:
                return []
                
        except Exception as e:
            print(f"âŒ Serper æœç´¢å¤±è´¥: {e}")
            return []

class WikipediaSearch:
    """
    Wikipedia API - æ— éœ€ Keyï¼ŒçŸ¥è¯†æŸ¥è¯¢
    """
    
    API_URL = "https://zh.wikipedia.org/w/api.php"
    
    def search(self, query: str, count: int = 10) -> List[Dict]:
        """æœç´¢ Wikipedia"""
        params = {
            "action": "query",
            "list": "search",
            "srsearch": query,
            "format": "json",
            "srlimit": count,
        }
        
        headers = {
            "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }
        
        try:
            response = requests.get(
                self.API_URL,
                params=params,
                headers=headers,
                proxies=PROXIES,
                timeout=15
            )
            
            if response.status_code == 200:
                data = response.json()
                results = []
                for item in data.get("query", {}).get("search", []):
                    title = item.get("title", "")
                    results.append({
                        "title": title,
                        "url": f"https://zh.wikipedia.org/wiki/{title.replace(' ', '_')}",
                        "description": item.get("snippet", "").replace("<span class='searchmatch'>", "**").replace("</span>", "**"),
                    })
                return results
            return []
            
        except Exception as e:
            print(f"âŒ Wikipedia æœç´¢å¤±è´¥: {e}")
            return []

class WebFetch:
    """
    ç½‘é¡µå†…å®¹æŠ“å–ï¼ˆå¸¦åçˆ¬ä¼ªè£…ï¼‰
    """
    
    def fetch(self, url: str) -> Optional[str]:
        """æŠ“å–ç½‘é¡µå†…å®¹"""
        try:
            headers = get_random_headers()
            headers["Accept"] = "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8"
            
            response = requests.get(
                url,
                headers=headers,
                proxies=PROXIES,
                timeout=20,
                allow_redirects=True
            )
            
            if response.status_code == 200:
                return response.text
            else:
                print(f"âš ï¸ HTTP {response.status_code}")
                return None
                
        except Exception as e:
            print(f"âŒ æŠ“å–å¤±è´¥: {e}")
            return None

class UnifiedSearch:
    """
    ç»Ÿä¸€æœç´¢æ¥å£ - è‡ªåŠ¨é€‰æ‹©å¯ç”¨çš„æœç´¢æº
    """
    
    def __init__(self):
        self.engines = {
            "brave": BraveSearch(),
            "serper": SerperSearch(),
            "wikipedia": WikipediaSearch(),
        }
        self.fetcher = WebFetch()
    
    def search(self, query: str, count: int = 10) -> Dict[str, List[Dict]]:
        """
        ä½¿ç”¨æ‰€æœ‰å¯ç”¨å¼•æ“æœç´¢
        
        Returns:
            {å¼•æ“å: ç»“æœåˆ—è¡¨}
        """
        results = {}
        
        for name, engine in self.engines.items():
            try:
                r = engine.search(query, count)
                if r:
                    results[name] = r
                    print(f"âœ… {name}: {len(r)} æ¡ç»“æœ")
                else:
                    print(f"âš ï¸ {name}: æ— ç»“æœ")
            except Exception as e:
                print(f"âŒ {name}: {e}")
        
        return results
    
    def fetch_article(self, url: str) -> Optional[str]:
        """æŠ“å–æ–‡ç« å†…å®¹"""
        return self.fetcher.fetch(url)


def demo():
    """æ¼”ç¤º"""
    print("ğŸ” å®ç”¨æœç´¢å·¥å…·æ¼”ç¤º\n")
    print("=" * 60)
    
    searcher = UnifiedSearch()
    
    # æµ‹è¯• Wikipediaï¼ˆä¸éœ€è¦ API Keyï¼‰
    print("\nğŸ“š Wikipedia æœç´¢: 'gold'\n")
    wiki = WikipediaSearch()
    results = wiki.search("gold", count=5)
    
    for i, item in enumerate(results, 1):
        print(f"{i}. {item['title']}")
        print(f"   {item['url']}")
        print()
    
    print("=" * 60)
    print("\nğŸ’¡ è¦å¯ç”¨æ›´å¤šæœç´¢æºï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡:")
    print("  export BRAVE_API_KEY=your_key_here")
    print("  export SERPER_API_KEY=your_key_here")
    print("\n  Brave Search: https://api.search.brave.com/")
    print("  Serper.dev: https://serper.dev/")

if __name__ == "__main__":
    demo()
